1
00:00:10,284 --> 00:00:13,524
L'intelligenza artificiale
al nostro servizio?

2
00:00:14,366 --> 00:00:16,563
L'IA è presente ovunque intorno a noi

3
00:00:16,588 --> 00:00:19,861
e fa sorgere domande di ordine sociale,
democratico, etico...

4
00:00:19,900 --> 00:00:20,978
e politico.

5
00:00:21,406 --> 00:00:23,320
È al servizio di chi? Di cosa?

6
00:00:23,345 --> 00:00:24,382
Chi controlla chi?

7
00:00:24,407 --> 00:00:26,275
Che ruolo le lasciamo ricoprire?

8
00:00:26,368 --> 00:00:28,461
In breve, in che società vogliamo vivere?

9
00:00:28,486 --> 00:00:32,095
L'aspetto appassionante della ricerca
sull'intelligenza artificiale

10
00:00:32,120 --> 00:00:33,499
è che permette di avanzare

11
00:00:33,524 --> 00:00:35,663
anche sulla comprensione
dell'intelligenza umana.

12
00:00:35,743 --> 00:00:39,109
Parallelamente, le discussioni
che suscita questa tecnologia

13
00:00:39,134 --> 00:00:42,048
ci spingono a una riflessione
sul comportamento dell'umanità,

14
00:00:42,196 --> 00:00:44,977
sul nostro ruolo di cittadini,
sulla nostra società.

15
00:00:45,002 --> 00:00:47,442
È un argomento ampio,
che merita la vostra attenzione.

16
00:00:47,467 --> 00:00:48,782
Ci riflettiamo insieme?

17
00:00:49,366 --> 00:00:51,944
Emozioni e creatività

18
00:00:52,326 --> 00:00:56,012
Per prima cosa, vorrei esaminare
la relazione tra umani e macchine.

19
00:00:56,037 --> 00:00:57,684
Per simulare l'intelligenza umana,

20
00:00:57,709 --> 00:01:00,004
l'intelligenza artificiale
tenta di riprodurre

21
00:01:00,029 --> 00:01:01,779
le funzioni cognitive degli umani:

22
00:01:01,804 --> 00:01:05,699
il ragionamento, la motricità,
il linguaggio, la vista, ma non solo.

23
00:01:05,724 --> 00:01:09,121
Anche usare la creatività
fa parte di queste funzioni cognitive.

24
00:01:09,146 --> 00:01:11,387
E al giorno d'oggi,
anche in questo momento,

25
00:01:11,412 --> 00:01:14,737
delle intelligenze artificiali
generano musica, scrivono libri,

26
00:01:14,762 --> 00:01:16,457
sceneggiature di film, dipingono...

27
00:01:16,567 --> 00:01:18,051
E fanno anche le pizze!

28
00:01:18,187 --> 00:01:20,744
E i risultati possono essere stupefacenti.

29
00:01:20,769 --> 00:01:22,916
Per quanto riguarda la pizza, può darsi,

30
00:01:22,941 --> 00:01:25,055
risparmio il mio giudizio,
non l'ho assaggiata.

31
00:01:25,080 --> 00:01:27,846
Eppure, si può dire che un'IA è creativa?

32
00:01:27,926 --> 00:01:31,324
Intanto, devo dire che siamo
ancora lontani da Hergé o dai Beatles.

33
00:01:31,349 --> 00:01:34,861
Le IA che progettiamo attualmente
non sono in grado di inventare,

34
00:01:34,886 --> 00:01:36,766
tuttavia, sono in grado di imitare.

35
00:01:36,791 --> 00:01:38,191
Secondo Albertine Meunier,

36
00:01:38,216 --> 00:01:41,336
artista digitale
che utilizza Internet come materiale,

37
00:01:41,361 --> 00:01:43,261
l'IA non è che uno strumento,

38
00:01:43,286 --> 00:01:47,399
ma propone un'infinità di possibilità,
di nuove forme e di opzioni

39
00:01:47,424 --> 00:01:49,075
alle quali non avremmo pensato.

40
00:01:49,200 --> 00:01:51,551
E queste macchine
possono provare emozioni?

41
00:01:51,576 --> 00:01:54,544
Le macchine possono comportarsi
come noi solo all'apparenza.

42
00:01:54,569 --> 00:01:58,324
Enormi progressi sono stati fatti
nel riconoscere le emozioni umane

43
00:01:58,349 --> 00:02:01,144
e reagire di conseguenza
con l'emozione appropriata.

44
00:02:01,169 --> 00:02:03,026
Ma, come ci dice Laurence Devillers,

45
00:02:03,051 --> 00:02:06,526
direttrice della ricerca del LIMSI/CNRS,

46
00:02:06,551 --> 00:02:08,909
le macchine non sentono nulla, simulano.

47
00:02:08,942 --> 00:02:11,760
È possibile portare la gente
a credere che sentano qualcosa

48
00:02:11,785 --> 00:02:13,822
utilizzando dei sensori, ad esempio.

49
00:02:14,230 --> 00:02:15,612
Se accarezzate il sensore,

50
00:02:15,637 --> 00:02:17,931
viene rilevato un segnale
e la macchina reagisce.

51
00:02:17,956 --> 00:02:20,064
In realtà, non abbiamo realmente bisogno

52
00:02:20,089 --> 00:02:21,962
di avere un robot intorno al programma.

53
00:02:21,987 --> 00:02:23,877
È sufficiente la voce a ingannarci,

54
00:02:23,902 --> 00:02:26,251
basta guardare l'effetto
degli assistenti vocali.

55
00:02:26,276 --> 00:02:28,502
"Fra 300 metri, girare a sinistra."

56
00:02:28,527 --> 00:02:31,722
Non possiamo negare
la nostra propensione all'antropomorfismo,

57
00:02:31,747 --> 00:02:33,107
che è molto potente.

58
00:02:33,308 --> 00:02:34,900
Se mi pongo tutte queste domande,

59
00:02:34,925 --> 00:02:36,245
è perché ritengo necessario

60
00:02:36,270 --> 00:02:38,848
interrogarsi sulle relazioni
che avremo con queste macchine

61
00:02:38,873 --> 00:02:41,090
e, soprattutto,
quanto spazio lasceremo loro.

62
00:02:41,486 --> 00:02:43,832
Supporto decisionale?

63
00:02:44,366 --> 00:02:47,287
Gli algoritmi dell'intelligenza
artificiale non fanno altro

64
00:02:47,312 --> 00:02:50,512
che riprodurre meccanicamente
dei compiti al nostro posto.

65
00:02:50,591 --> 00:02:53,826
Possono essere strumenti formidabili
di supporto alle decisioni

66
00:02:53,851 --> 00:02:54,897
in parecchi settori,

67
00:02:54,922 --> 00:02:58,765
come i trasporti, i servizi bancari,
le assicurazioni, la sicurezza...

68
00:02:58,790 --> 00:03:02,101
Un campo particolarmente interessante
è quello della medicina.

69
00:03:02,200 --> 00:03:05,637
Per esempio, gli algoritmi
possono aiutare a diagnosticare un tumore

70
00:03:05,662 --> 00:03:07,973
analizzando le radiografie.

71
00:03:08,339 --> 00:03:12,141
E in futuro, magari, gli algoritmi
potranno diagnosticare un'appendicite

72
00:03:12,166 --> 00:03:14,000
o prevedere la comparsa di un tumore

73
00:03:14,025 --> 00:03:16,126
prima ancora
che sia visibile dalle immagini.

74
00:03:16,228 --> 00:03:17,603
È formidabile!

75
00:03:17,753 --> 00:03:18,955
È geniale!

76
00:03:19,406 --> 00:03:21,087
Sì, finché va tutto bene.

77
00:03:21,279 --> 00:03:23,806
Gli algoritmi possono sempre
avere dei difetti.

78
00:03:24,029 --> 00:03:26,332
Niente di grave
finché si tratta di decisioni

79
00:03:26,357 --> 00:03:28,435
e suggerimenti sugli spazzolini da denti,

80
00:03:28,638 --> 00:03:31,825
ma se tali decisioni hanno un impatto
sulla mia vita personale

81
00:03:31,850 --> 00:03:33,912
in campo medico o giudiziario?

82
00:03:34,892 --> 00:03:37,225
Non ci si può accontentare
di risposte come:

83
00:03:37,250 --> 00:03:40,514
"Mi spiace, signore, non dipende da me.
È la macchina che ha deciso."

84
00:03:40,539 --> 00:03:41,539
No!

85
00:03:41,789 --> 00:03:45,261
È indispensabile poter fornire
delle spiegazioni e delle giustificazioni

86
00:03:45,286 --> 00:03:48,617
sui consigli che vengono dati
o le decisioni prese, no?

87
00:03:48,886 --> 00:03:51,573
È tutta questione
di quella che chiamiamo "spiegabilità".

88
00:03:51,598 --> 00:03:53,737
La tecnica più in voga al giorno d'oggi,

89
00:03:53,762 --> 00:03:57,237
l'apprendimento attraverso reti neurali
profonde, o <i>deep learning</i>,

90
00:03:57,262 --> 00:03:59,215
funziona un po' come una scatola nera.

91
00:03:59,240 --> 00:04:00,707
Non sappiamo davvero spiegare

92
00:04:00,732 --> 00:04:02,824
come le reti neurali
arrivino ai risultati,

93
00:04:02,849 --> 00:04:05,455
almeno non in modo comprensibile
per gli esseri umani.

94
00:04:05,480 --> 00:04:08,798
Sono in corso delle ricerche
per rendere questi sistemi più trasparenti

95
00:04:08,823 --> 00:04:11,501
e sono stati fatti enormi progressi
in questo campo.

96
00:04:11,526 --> 00:04:15,620
Alexei Grinbaum,
filosofo della scienza CERNA e fisico CEA,

97
00:04:15,674 --> 00:04:17,995
spiega che è necessario
progettare un sistema

98
00:04:18,020 --> 00:04:21,516
in modo tale che sia sempre possibile
ricostruire le catene causali

99
00:04:21,541 --> 00:04:24,704
che hanno portato a una qualsiasi
azione o decisione della macchina.

100
00:04:24,729 --> 00:04:27,680
Ad esempio, attraverso
un altro sistema di apprendimento

101
00:04:27,705 --> 00:04:29,606
che spieghi cosa fa il primo sistema,

102
00:04:29,631 --> 00:04:32,588
offrendo un primo approccio
a cosa succede al suo interno.

103
00:04:32,713 --> 00:04:36,154
L'effetto scatola nera
cela l'origine dei pregiudizi.

104
00:04:36,179 --> 00:04:39,754
I pregiudizi provengono dai dati
serviti per addestrare il programma,

105
00:04:39,779 --> 00:04:42,261
ma che diventa difficile
analizzare in seguito.

106
00:04:42,286 --> 00:04:45,714
Servono migliaia e migliaia di dati
al programma per imparare.

107
00:04:45,739 --> 00:04:49,425
Come si dice? Impara velocemente,
ma bisogna spiegarglielo a lungo.

108
00:04:49,613 --> 00:04:51,824
Dati non molto numerosi,
non volto vari,

109
00:04:51,849 --> 00:04:54,455
o mal etichettati,
possono portare a risultati sbagliati.

110
00:04:54,480 --> 00:04:56,245
Ad esempio, se si addestra un algoritmo

111
00:04:56,270 --> 00:04:58,095
a riconoscere una pecora in un'immagine...

112
00:04:58,120 --> 00:04:58,316
Bè!

113
00:04:58,341 --> 00:04:59,862
...e gli si mostrano solo pecore nere,

114
00:04:59,887 --> 00:05:01,713
non potrà riconoscere le pecore bianche.

115
00:05:01,738 --> 00:05:03,651
E poi si dice che l'IA è razzista!

116
00:05:04,041 --> 00:05:06,385
Infatti, i dati sono distorti
dai nostri stereotipi

117
00:05:06,410 --> 00:05:08,183
e una macchina addestrata
con dati distorti

118
00:05:08,208 --> 00:05:11,425
riprodurrà gli stessi pregiudizi,
senza neanche comprenderli.

119
00:05:11,465 --> 00:05:12,665
Bè!

120
00:05:12,752 --> 00:05:16,236
Uno studio pubblicato
dalla rivista "Science", nel 2017,

121
00:05:16,261 --> 00:05:19,423
ha dimostrato che molti pregiudizi
esistono nel linguaggio umano.

122
00:05:19,448 --> 00:05:20,979
Ad esempio, la parola "uomo"

123
00:05:21,004 --> 00:05:24,165
è spesso associata a termini
riferiti a posizioni da dirigente,

124
00:05:24,190 --> 00:05:26,290
nel settore scientifico o ingegneristico,

125
00:05:26,315 --> 00:05:27,557
mentre la parola "donna"

126
00:05:27,582 --> 00:05:29,589
è associata a posizioni di assistente

127
00:05:29,614 --> 00:05:31,292
nel settore artistico e umanistico.

128
00:05:31,317 --> 00:05:33,621
Allo stesso modo, i nomi afroamericani

129
00:05:33,646 --> 00:05:35,955
rimandano a termini
tutt'altro che lusinghieri.

130
00:05:35,980 --> 00:05:37,808
Dunque, se non si fa attenzione,

131
00:05:37,833 --> 00:05:41,002
quando toccherà a un'IA scegliere
un candidato o una candidata

132
00:05:41,027 --> 00:05:42,526
per un posto da ingegnere,

133
00:05:42,551 --> 00:05:45,159
il CV di una donna nera
finirà in fondo alla pila.

134
00:05:45,566 --> 00:05:49,158
Serge Abiteboul, ricercatore
del Dipartimento d'Informatica

135
00:05:49,183 --> 00:05:50,806
dell'École normale supérieure,

136
00:05:50,831 --> 00:05:53,821
ci racconta un'esperienza sbalorditiva
fatta in Israele.

137
00:05:53,846 --> 00:05:54,955
Ci si è resi conto

138
00:05:54,980 --> 00:05:58,350
che le probabilità di essere liberati,
passando davanti a un giudice,

139
00:05:58,375 --> 00:06:00,853
erano più alte dopo pranzo, che prima.

140
00:06:01,620 --> 00:06:04,627
Questo mi lascia perplesso
sui miei stessi criteri decisionali.

141
00:06:04,966 --> 00:06:07,697
Beh, per fortuna che ci sono
altri fattori decisionali,

142
00:06:07,722 --> 00:06:09,381
oltre allo stato del loro stomaco.

143
00:06:09,406 --> 00:06:10,721
In ogni caso, si può dire

144
00:06:10,746 --> 00:06:13,546
che l'IA riproduce sempre
lo stesso comportamento,

145
00:06:13,620 --> 00:06:15,620
a partire dai dati che le vengono forniti.

146
00:06:16,011 --> 00:06:18,411
Da qui, l'importanza dei dati,
ancora una volta,

147
00:06:18,436 --> 00:06:20,862
e delle motivazioni
di chi progetta queste macchine.

148
00:06:21,286 --> 00:06:24,317
Questione di democrazia

149
00:06:24,566 --> 00:06:28,246
L'eterna questione degli strumenti
e di cosa si debba fare con essi.

150
00:06:28,326 --> 00:06:31,927
Sono gli esseri umani a definire
gli obiettivi di questi programmi.

151
00:06:32,246 --> 00:06:35,446
Se ne può discutere con loro,
anzi, è indispensabile.

152
00:06:35,486 --> 00:06:37,166
Non sono altro che filantropi!

153
00:06:37,206 --> 00:06:40,330
Detto questo, anche quando si hanno
le migliori intenzioni del mondo,

154
00:06:40,355 --> 00:06:41,509
la domanda resta:

155
00:06:41,534 --> 00:06:42,574
chi deve decidere?

156
00:06:43,046 --> 00:06:46,946
Serge Abiteboul risponde
che certamente non spetta agli informatici

157
00:06:46,971 --> 00:06:48,993
decidere come mettere a punto l'algoritmo

158
00:06:49,018 --> 00:06:50,844
che calcola le sorti dei laureati,

159
00:06:50,869 --> 00:06:52,375
così come non spetta a Google

160
00:06:52,400 --> 00:06:55,142
decidere di vietare
siti estremisti o di fake news.

161
00:06:55,167 --> 00:06:57,848
Il mondo digitale
si è sviluppato così velocemente

162
00:06:57,873 --> 00:06:59,444
che è ancora nell'era del Far West:

163
00:06:59,469 --> 00:07:00,789
le ingiustizie abbondano,

164
00:07:00,814 --> 00:07:03,743
lo Stato non è ancora in grado
di legiferare correttamente

165
00:07:03,768 --> 00:07:05,448
e i cittadini si sentono persi.

166
00:07:05,473 --> 00:07:07,931
Numerosi ricercatori
sono d'accordo su un punto:

167
00:07:07,956 --> 00:07:11,071
le macchine possono aiutarci
a far maturare le nostre decisioni

168
00:07:11,096 --> 00:07:13,136
proponendoci motivazioni e soluzioni,

169
00:07:13,163 --> 00:07:14,407
ma spetta agli esseri umani

170
00:07:14,432 --> 00:07:17,061
decidere quali regole
devono seguire queste macchine

171
00:07:17,086 --> 00:07:19,923
e, in caso di dubbi,
prendere la decisione finale.

172
00:07:20,407 --> 00:07:22,532
Allora, che si fa
mentre si aspetta lo sceriffo?

173
00:07:22,557 --> 00:07:25,453
Ci si rivolge alla coscienza
di coloro che creano gli algoritmi,

174
00:07:25,478 --> 00:07:26,789
si creano regole etiche

175
00:07:26,814 --> 00:07:28,118
e si informano i cittadini,

176
00:07:28,143 --> 00:07:30,460
affinché possano mobilitarsi,
se necessario.

177
00:07:30,485 --> 00:07:31,921
Ecco cosa stiamo facendo!

178
00:07:31,946 --> 00:07:33,249
Sì, Théo, esattamente,

179
00:07:33,274 --> 00:07:34,606
un piccolo contributo.

180
00:07:34,631 --> 00:07:37,001
In ogni caso, spero
che questo umile lavoro

181
00:07:37,026 --> 00:07:38,915
interamente realizzato dagli umani

182
00:07:38,940 --> 00:07:40,735
vi permetterà di vederci più chiaro.

183
00:07:40,966 --> 00:07:43,884
Sta a tutti noi
sfruttare l'intelligenza artificiale

184
00:07:43,978 --> 00:07:45,624
e di inventare nuovi utilizzi

185
00:07:45,649 --> 00:07:48,046
che ci renderanno la vita
più facile e più interessante.

186
00:07:48,251 --> 00:07:49,731
O semplicemente più bella.

